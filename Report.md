#Using Crowdsourced trained GIAL to achieve desirable BC in FPS games

Abstract

We present a novel framework for training human-like game-playing agents using a hybrid of reinforcement learning (RL) and imitation learning with crowd-sourced human data. Our agent model is a convolutional-recurrent policy trained with Proximal Policy Optimization (PPO) and guided by imitation rewards derived from human demonstrations. Key innovations include (1) a hybrid training loop that alternates between human-controlled and agent-controlled rollouts to collect diverse behaviors, (2) incorporating an exponential moving average (EMA) of recent actions into the agent’s input to capture temporal behavior traces, (3) a sequence-aware reward network (CNN+GRU discriminator) that scores how well agent trajectories match human play and mitigates spurious discriminator biases, and (4) a planned multi-agent Battle Royale tournament for large-scale evaluation of performance and human-likeness. We describe the agent and reward network architectures, our crowd-sourcing protocol for data collection, and a comprehensive evaluation plan using gameplay metrics and human assessments. This work is aimed at academic research on imitation and multi-agent learning, providing a reproducible environment and methodology for crowd-sourced training of believable game agents.

Introduction

Modern video games often require AI agents to perform competently under tight computational constraints while also exhibiting human-like behavior for a compelling player experience ￼. In multiplayer shooter games, for example, it is not enough for an agent to simply optimize win-rate; it must also move, react, and strategize in ways that are believable to human players. Striking this balance is challenging: academic RL agents typically maximize game score, whereas game designers seek agents that imitate human play styles ￼.

The open-source StrikeForce environment (a simple battle-royale arena) provides a platform for developing such agents ￼. We leverage crowd-sourced human data and advances in imitation learning to train agents that are both effective and human-like. Specifically, we build on generative imitation frameworks and PPO-based RL: our agent uses a CNN+GRU policy network updated via PPO, combined with imitation rewards from a learned discriminator. Crucially, we alternate between human-driven and agent-driven rollouts in training to blend expert demonstrations with autonomous exploration. We also augment the agent’s state with an EMA of past actions to encode recent behavior history (capturing stylistic “momentum”). Our discriminator (RewardNet) is also recurrent, evaluating entire action sequences to avoid exploiting trivial statistical cues ￼. Finally, we plan to evaluate agents in large-scale multi-agent tournaments (Battle Royale), measuring both gameplay success and authenticity. This paper details the architecture and training protocol of this system.

The main contributions of this work are:
	•	Hybrid imitation-RL agent: A CNN+GRU policy network trained with PPO updates and imitation reward signals, combining human demonstrations with agent exploration.
	•	Behavioral trace encoding: Inclusion of an exponential moving average of recent actions in the agent’s input state to encode temporal behavior patterns.
	•	Sequence-aware discriminator: A CNN–GRU reward network that scores sequences of states/actions, mitigating the tendency of adversarial discriminators to latch onto spurious features ￼.
	•	Crowd-sourced training pipeline: A protocol for gathering human gameplay data and mixing it into training via partially human- and agent-controlled episodes.
	•	Multi-agent evaluation framework: A planned Battle Royale tournament architecture for assessing agent performance and human-likeness at scale, inspired by multi-agent RL competitions ￼ ￼.

Related Work

Imitation Learning: Imitation learning (IL) trains agents to mimic human or expert behavior by learning a mapping from observations to actions ￼. Standard approaches include behavior cloning and more sophisticated methods like DAgger. However, IL alone often “exploits” the limited demonstration data, suffering from compounding errors and poor exploration ￼. In games this can lead to agents that appear rigid or overfit to the training data. To address this, generative adversarial imitation methods such as GAIL have been proposed, which frame imitation as an adversarial game between a policy and a discriminator ￼. These methods can yield more robust, naturalistic behavior by continually engaging the policy in exploration ￼. Nonetheless, adversarial IL is known to have vulnerabilities: discriminators may exploit trivial or spurious patterns in the data, focusing on irrelevant features of observations rather than the underlying task ￼. This motivates our use of a sequence-aware discriminator and additional context input.

Sequential and Memory-based Models: Game playing is inherently sequential, and temporal context can be crucial. Recent IL work employs recurrent architectures (LSTMs/GRUs) or transformers to capture long-range dependencies in action sequences ￼ ￼. For example, recurrent BC was used in Counter-Strike bot cloning ￼. Our agent likewise uses a GRU layer to maintain state across time. Unlike trend toward huge transformer models, we focus on compact networks suitable for real-time play ￼.

Hybrid Human-in-the-Loop Training: Gathering human demonstrations can be expensive. Crowdsourcing has been proposed to accelerate IL by leveraging many users online ￼. Chung et al. show that crowdsourced demonstrations can enrich a robot’s training data, overcoming data scarcity ￼. Similarly, systems like RoboTurk enable large-scale teleoperation and human interventions in RL training ￼. In RoboTurk, humans can both demonstrate tasks and intervene while an agent learns ￼. Our protocol draws on these ideas: we collect human gameplay data in StrikeForce and allow human players to either fully control episodes or intervene in agent play. We effectively use a DAgger-like scheme: the agent explores, but when uncertainties arise, human experts step in to guide it. The resulting mixed dataset is used for both supervised imitation and adversarial reward training.

Multi-Agent and Tournament Learning: Training agents in multi-agent environments often benefits from self-play and league methods. DeepMind’s AlphaStar achieved grandmaster StarCraft II play by combining self-play and a structured league of agents ￼. League training alternates the learning agent between competing against past copies, specialized exploiters, and the human games, improving robustness and diversity ￼. We adopt a similar spirit for evaluation: by staging a Battle Royale tournament with many agents, we probe the strengths and styles of learned policies. We will track metrics like win-rate and Elo ratings across a diverse league of agents. Unlike top-tier systems that prioritize winning, our focus is on believability: measuring how often agents fool human judges or match distributions of human actions. Prior work on believability uses human Turing tests and behavioral metrics ￼; we will employ analogous multifaceted assessments.

Method

Agent Architecture

Our agent (implemented in Agent.hpp) is a deep recurrent policy network. The observation space includes a local grid-based state (e.g. positions of nearby objects and opponents) and auxiliary variables (health, ammunition). This is processed by a CNN that applies 2D convolutions (with BatchNorm and ReLU) and global average pooling to produce a latent feature vector. Formally,
$$h_0 = \text{CNN}(o_t),,$$
where $o_t$ is the raw observation at time $t$ and $h_0\in\mathbb{R}^d$ is a fixed-size embedding. In parallel, we maintain an exponentially smoothed history of past actions $a_{t-1},a_{t-2},\dots$ as an additional input channel. This EMA of recent actions captures the agent’s recent behavioral trace, giving the network information about momentum or patterns in play.

The embedded state $h_0$ is fed into a gated recurrent unit (GRU) with hidden size $d$. The GRU updates an internal hidden state $h_t = \text{GRU}(h_0, h_{t-1})$, which carries temporal context from past observations and actions. We use a 2-layer GRU in our experiments. From the GRU output, two linear “heads” predict a policy and value:
$$\pi(\cdot|h_t) = \text{Softmax}(W_\pi h_t + b_\pi),,\quad V(h_t) = W_V h_t + b_V,,$$
where $W_\pi\in\mathbb{R}^{|\mathcal{A}|\times d}$ and $W_V\in\mathbb{R}^{1\times d}$ (with $|\mathcal{A}|$ the number of discrete actions). This actor-critic setup allows using PPO for updates. We follow standard PPO training: interactions yield trajectories of $(o_t,a_t,r_t)$, from which we compute discounted returns and advantages, and perform multiple minibatch gradient ascent steps on the PPO surrogate loss ￼. The policy and value networks are updated simultaneously (value updated with mean-squared error to empirical returns, policy with clipped objective).

To incorporate imitation from human input, we augment the agent’s loss with an imitation reward $r_t^\text{im}$. This reward comes from our discriminator network (RewardNet; see below) and encourages the agent to produce trajectories that the network deems “human-like”. Concretely, the agent’s total reward is
$$r_t^\text{total} = r_t^\text{env} + \lambda,r_t^\text{im},$$
where $r_t^\text{env}$ is the native game reward (e.g. elimination, survival) and $\lambda$ is a weighting coefficient. PPO training maximizes these combined rewards. In practice, we set $\lambda$ to balance imitation and task objectives empirically.

Reward Network (Discriminator)

The RewardNet (RewardNet.hpp) is trained adversarially to distinguish human demos from agent rollouts, providing a learned reward signal for imitation. It has a similar CNN–GRU backbone. Given a sequence of recent observations and actions, the CNN processes each observation frame (and includes the EMA action channel) to produce embeddings $h_0^t$. These embeddings are fed sequentially into a GRU to produce an aggregated hidden state $H$, which is then passed through a linear layer and sigmoid to output a probability of “human origin”:
$$p_\text{human} = \sigma(W_r H + b_r),. $$
We interpret $r^\text{im} = \log p_\text{human}$ as the imitation reward (higher if trajectory looks human). Training alternates between collecting trajectories from the current agent and from human demonstrations. The RewardNet is then trained as a binary classifier (cross-entropy loss) to predict the source of each sequence.

A critical design choice is that the RewardNet evaluates sequences of behavior rather than single frames. This sequence-aware design addresses known adversarial imitation pitfalls: as observed in prior work, discriminators can overfit to frame-level artefacts and learn spurious visual cues ￼. By incorporating temporal context via the GRU, the discriminator can focus on higher-level behavioral patterns, mitigating bias. In effect, the RewardNet looks at, say, the consistency of aiming, movement patterns, or reaction timing over a short time window, rather than isolated snapshots.

EMA Action Input

The exponential moving average (EMA) of past actions is encoded as an additional input channel in both the agent and discriminator networks. Formally, let $\bar{a}t = \alpha,a_t + (1-\alpha),\bar{a}{t-1}$ (vector of action one-hot or real-valued proxies). This vector $\bar{a}_t$ is included alongside sensory inputs. Intuitively, $\bar{a}_t$ summarizes the recent action history, providing a “behavioral trace” that the networks can use. For the agent, this means it knows how its recent actions have evolved. For the RewardNet, it helps in recognizing style (e.g., whether an agent keeps strafing or changes direction frequently). This approach is akin to providing an eligibility trace or momentum in the input, enriching the state representation with temporal structure.

Crowd-Sourced Training Protocol

Our training protocol leverages human players to seed and guide agent learning. The high-level loop is as follows:
	1.	Human Demonstrations: Recruit volunteer players to play StrikeForce matches. Collect logged trajectories of their gameplay (state observations and actions). Multiple players ensure diversity of play styles.
	2.	Agent Rollouts with Human Interventions: Train the agent for a number of episodes using PPO with the combined reward. After each training phase, allow humans to intervene in agent games: for example, the agent plays but a human can take over control if the agent is stuck. These interventions generate additional labelled data (the human’s corrective actions). This is similar in spirit to DAgger and to the Intervention Weighted Regression idea in RoboTurk ￼.
	3.	Discriminator Updates: Periodically, use the collected human data versus the latest agent rollouts to train the RewardNet. This classifier learns to distinguish human vs agent play sequences, updating its weights to provide a moving imitation reward.
	4.	Iterate: Alternate between (a) collecting new data (from human games and mixed human-agent games) and (b) updating the agent with PPO (using the updated RewardNet). Over time, the agent sees more variations of human behavior and gradually learns to mimic it.

Optionally, we maintain a buffer of recent human and agent trajectories to stabilize training. The mixing ratio of human vs agent rollouts is a parameter; we expect gradually shifting weight toward more agent self-play as performance improves, to encourage exploration beyond initial demonstrations. The crowdsourced nature means that many humans can contribute asynchronously, speeding up data collection ￼.

In practice, our bots/bot-1/Custom.hpp code orchestrates human player connections and alternation. It uses a simple flag to decide at the start of each episode whether the agent or a human will drive the player character (or a blend via intervention). This hybrid control strategy ensures that the agent is exposed to challenging states (from human play) and learns recovery actions from interventions, which can improve robustness compared to purely offline BC.

Evaluation Plan and Metrics

We evaluate agents on two fronts: performance (gameplay success) and human-likeness. To measure performance, we organize a large-scale Battle Royale tournament: many agent instances (with different random seeds and training checkpoints) and human proxies play in a free-for-all arena until one remains. We record metrics such as win rates, average eliminations, survival time, and assign Elo ratings to each agent. This multi-agent tournament setup is inspired by league training in StarCraft II ￼, though on a smaller scale. It reveals how strategies evolve and whether agents exploit the game mechanics or fail to adapt to each other.

For human-likeness, we use both quantitative and qualitative metrics. Quantitatively, we track the discriminator’s accuracy over time: as training proceeds, a strong agent will increasingly fool the RewardNet, driving its human-vs-agent classification closer to chance. We also compute statistical measures comparing action distributions of agents versus humans (e.g. Bhattacharyya coefficient on action frequencies, as in human-versus-bot evaluation ￼). Qualitatively, we will conduct user studies where human judges view gameplay clips (some from agents, some from humans) and rate how human-like the agent play appears. This is akin to Turing-like evaluations used in prior work ￼.

Evaluation is carried out periodically during training. Early on, we expect agents to play poorly and unnaturally; as training progresses, both game rewards and imitation metrics should improve. Crucially, we also compare against baselines: (i) a purely RL-trained agent (no imitation loss), (ii) a purely behavior-cloned agent (no RL), and (iii) an agent with the same architecture but no sequence-awareness in the discriminator. These comparisons isolate the effects of our hybrid approach.

Discussion and Limitations

Our approach builds on known IL and RL methods but integrates them in a novel crowdsourced setting. Nonetheless, there are limitations. The quality of the final agent is tied to the quality of human data; if demonstrations are suboptimal or biased, the agent may learn those biases ￼. The discriminator network, despite being sequence-aware, may still latch onto unintended cues in the data – for instance, if humans always take a certain position in the map, the network might reward that without the agent truly learning strategy. This is a well-known issue in adversarial IL, where discriminators can exploit spurious correlations ￼. Our EMA input and sequence modeling are designed to mitigate this, but careful validation is needed.

Training stability is another concern. PPO training with learned rewards can be sensitive; reward shaping (via $\lambda$) must be tuned to balance task success with imitation fidelity. Too much imitation pressure can cause the agent to “play safe” and not explore novel tactics, while too little may make it effective but robotic. We address this by gradually annealing $\lambda$ and monitoring both game score and imitation score.

Finally, our evaluation focuses on a specific game environment (StrikeForce). Results may not generalize directly to other games or even more complex shooter scenarios. We have chosen relatively small CNN+GRU models to ensure real-time performance; more complex perception (e.g. vision-based) is left for future work. Scalability to many agents also depends on computational resources, but our Battle Royale design can be parallelized.

Conclusion and Future Work

We have outlined a comprehensive system for training human-like agents via crowdsourced imitation and reinforcement learning. By combining expert demonstrations with autonomous learning (hybrid rollouts), enriching agent inputs (EMA of actions), and using a sequence-informed discriminator, our approach aims to produce agents that both perform well and appear natural. Planned multi-agent tournament evaluations will benchmark these agents on performance and believability.

Future work includes expanding the variety of human demonstrations (e.g. different skill levels or playstyles), exploring more sophisticated sequence models (e.g. transformers), and integrating curriculum learning to handle harder scenarios. We also plan to open-source our training framework and benchmark tournaments to facilitate research on crowd-sourced game AI. Ultimately, this work contributes to the understanding of how human data and adversarial techniques can be combined at scale to produce credible, robust game agents.

References

[Imitation learning references] – Imitation learning is widely used to train agents from human demonstrations ￼, but it tends to “memorize” trajectories and can limit exploration ￼. Adversarial imitation methods like GAIL introduce a discriminator to improve robustness and diversity of policies ￼. However, discriminators can overfit to spurious features ￼, motivating our sequence-aware design.
[Crowdsourced training] – Crowdsourcing human demonstrations has proven effective in robotics and IL ￼ ￼, and our protocol similarly uses many volunteer players to generate data.
[Multi-agent training] – Multi-agent tournaments (e.g. AlphaStar’s league) produce diverse, strong agents by mixing self-play and varied opponents ￼. We adopt a scaled-down Battle Royale tournament for evaluation.
[PPO algorithm] – We use PPO for policy optimization, a simple yet powerful on-policy RL method ￼, balancing sample efficiency and ease of implementation.
[Evaluation] – Following prior work, we evaluate via direct match-ups and human-subject tests ￼ to gauge both skill and authenticity of gameplay.